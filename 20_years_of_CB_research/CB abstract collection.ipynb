{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping abstracts for central bank research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install Scrapy\n",
    "# !{sys.executable} -m pip install wordcloud\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from bs4 import NavigableString\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "import threading\n",
    "import csv\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function (1)\n",
    "**Return a list with a link to each paper**\n",
    "* NB: This is for a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup):\n",
    "    \n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"/p/\")}):\n",
    "        links.append(\"https://ideas.repec.org\"+link.get('href'))\n",
    "\n",
    "    return links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function (2)\n",
    "**Soup the necessary URL and call Function (1) to return the list of links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_links(START_URL):\n",
    "    \n",
    "    page   = requests.get(START_URL)\n",
    "    soup   = BeautifulSoup(page.text, 'html.parser')\n",
    "    links = get_links(soup)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function (3)  \n",
    "**For a given central bank, go through each page and collect the links by calling Function (2) for each page**\n",
    "\n",
    "1. use Function (2) to get all the links from the first page for the first central bank\n",
    "2. access that URL's .html **again**\n",
    "3. get the links that direct to the next pages i.e. [1],[2]... for that central bank\n",
    "4. if the last item in that list is not empty get the link to the next page\n",
    "5. return to 1. above using the link saved in 4.\n",
    "6. if there and no pages, quit the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_links(START_URL):\n",
    "    \n",
    "    items_list = [] \n",
    "    current_link = START_URL   # Setting up the starting link\n",
    "    \n",
    "    no_more_pages = False\n",
    "    while no_more_pages == False:\n",
    "        \n",
    "        items_list = items_list + parse_links(current_link)   # Add items from page to the list\n",
    "        request = requests.get(current_link)                   # Accessing text content again\n",
    "        parsed_request = BeautifulSoup(request.content, 'html.parser')   # Parsing it again\n",
    "        link = parsed_request.find_all('li', class_= \"page-item\")        # Finding next link from within tree\n",
    "        try:\n",
    "            if not link[-1].find('a')['href'] is False:\n",
    "                current_link = current_link.rsplit('/', 1)[0]+'/'+link[-1].find('a')['href']\n",
    "        except:\n",
    "            no_more_pages = True\n",
    "    items_list.pop(0)\n",
    "    items_list.pop(0)\n",
    "    return items_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = parse_all_links(\"https://ideas.repec.org/s/rba/rbardp.html\")         # RBA\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/fip/fedgfe.html\"))   # Fed\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/ecb/ecbwps.html\"))   # ECB\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/boj/bojwps.html\"))   # BoJ\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/bca/bocawp.html\"))   # Bank of Canada\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/nzb/nzbdps.html\"))   # RBNZ\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/boe/boeewp.html\"))   # BoE\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/hhs/rbnkwp.html\"))   # Riksbank\n",
    "links.extend(parse_all_links(\"https://ideas.repec.org/s/bno/worpap.html\"))   # Norges Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function (4) \n",
    "**For a given abstract page, pull the text of that abstract**\n",
    "\n",
    "NB: We need the except pass conditions:\n",
    "* before 1996 for the Fed (here: https://ideas.repec.org/s/fip/fedgfe9.html) the results are no longer interpretable by the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts(URL):\n",
    "    \n",
    "    abstracts = []\n",
    "    \n",
    "    page      = requests.get(URL)\n",
    "    soup      = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    abstract  = soup.find('div', id='abstract-body').get_text()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        info      = [desc.strip() for desc in \n",
    "                     soup.find(\"li\",class_=\"list-group-item downfree\").descendants if type(desc) == NavigableString]\n",
    "        \n",
    "        abstracts.append({'Author/s': info[0],'Year': info[0].split()[-2], \n",
    "                      'Title': info[1], 'ID': info[4], 'Abstract': abstract})\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call function (4) using concurrency via threading!\n",
    "* Give each thread a different task (URL)\n",
    "* In this case, we will handle all the tasks at once\n",
    "* This is great for the purpose at hand, as I have a list of pre-defined URLs to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB: Change the number of threads if the IDEAS page throws a request error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(72) as pool:\n",
    "      abstracts = pool.map(get_abstracts, links)\n",
    "\n",
    "abstracts = list(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_data_.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = abstracts[0][0].keys()\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for x in rangenge(0,len(abstracts)):\n",
    "        for row in abstracts[x]:\n",
    "            writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
